# Code Completion Evaluation

## Introduction

In this project, I aim to evaluate the performance of an open-source code completion model using code snippets from different repositories. By simulating the user's cursor position within the code, we can assess how well the model predicts the missing code segments.

## Dataset Preparation

### Selecting Code Files

I selected several files from several personal projects written in Python. These files encompass various functionalities, including data processing, API interactions, and utility functions.
Also I decided to find some publicly availible code snippets written in pure python (without any libraries and frameworks) as though I don't know the dataset the model was trained on.

### Splitting Code into Prefix, Middle, and Suffix

A script was developed to simulate the user's cursor position within the code. The script splits the code into three parts:

1. **Prefix**: Code before the cursor position.
2. **Middle**: The missing code segment that we assume should be typed next.
3. **Suffix**: Code after the cursor position.

There were 3 different approaches for split generations, `splitting.py`,`splitting_v2.py`,`splitting_v3.py`. At the end I stick with the approach that
ensured that the middle segment was meaningful, not splitting the words in half, as though messing this caused model to produce a lot of broken code. A total of **38** examples were generated for evaluation.



This script reads code files from the specified `CODEBASE` directory and generates splits by:

- Randomly selecting a cursor position within the code.
- Adjusting the cursor position to the nearest word boundary to avoid splitting tokens.
- Determining the length of the middle segment (the missing code) to be either short or long, simulating realistic code completions.
- Ensuring that none of the prefix, middle, or suffix are empty or trivial.

The splits are saved in a JSON file named `splits_v3.json`.

## Model Selection and Code Completion

I chose the **Tiny StarCoder Python Model** (`bigcode/tiny_starcoder_py`) for generating code completions due to its proficiency in understanding and generating Python code and the most important thing - **speed**.



This script:

- Loads the splits generated previously.
- For each split, it generates a code completion using the model.
- Computes various automatic metrics (they will be provided below) comparing the generated code to the actual missing code.
- Allows for manual labeling of the generated code as **correct**, **partially_correct**, or **incorrect**.
- Stores the results along with metrics in `annotated_completions_v3.json`.

## Manual Evaluation

Each generated code completion was manually reviewed and compared to the actual missing code (the middle segment). I assessed:

- **Correctness**: Does the generated code functionally match the expected code?
- **Syntax**: Is the generated code syntactically correct?
- **Relevance**: Does the code make sense in the given context?

Based on this review, I assigned labels:

- **correct**
- **partially_correct**
- **incorrect**

## Automatic Metrics Evaluation

To automate the evaluation process, I computed several metrics to compare the model's output against the actual missing code:

1. **Exact Match**: Binary indicator of whether the generated code exactly matches the missing code.
2. **chrF Score**: Character n-gram F-score, useful for assessing similarity at the character level.
3. **BLEU Score**: Measures the overlap between generated and reference text.
4. **ROUGE-L Score**: Focuses on the longest common subsequence.
5. **Levenshtein Ratio**: Measures the similarity between two strings based on edit distance.

These metrics were computed using libraries such as `sacrebleu`, `rouge_score`, and `python-Levenshtein`.

## Correlation Analysis

To determine which metric aligns best with human judgment, I calculated the Pearson and Spearman correlation coefficients between the automatic metrics and the manual labels.

### Results

The correlation coefficients are as follows:

#### 1. Exact Match

- **Pearson Correlation**: 0.3464
- **Spearman Correlation**: 0.3316

---

#### 2. chrF Score

- **Pearson Correlation**: 0.2609
- **Spearman Correlation**: 0.1880

---

#### 3. BLEU Score

- **Pearson Correlation**: 0.2403
- **Spearman Correlation**: 0.0775

---

#### 4. ROUGE-L Score

- **Pearson Correlation**: 0.1549
- **Spearman Correlation**: 0.1066

---

#### 5. Levenshtein Ratio

- **Pearson Correlation**: 0.2029
- **Spearman Correlation**: 0.0899

---

## Analysis

The **Exact Match** metric obviously showed the highest correlation with my judgment.

Other metrics like **chrF Score** and **BLEU Score** also showed positive correlations. This indicates that while these metrics capture some aspects of similarity, they may not fully align with human perceptions of code correctness, but they're the most acceptable for automated labeling.

The lower correlations for **ROUGE-L Score** and **Levenshtein Ratio** suggest that these metrics may not be as effective in evaluating code completions, possibly due to the specific nature of code syntax and structure.

## Conclusion

The evaluation demonstrates that automatic metrics can provide insights into the model's performance, but they may not entirely reflect human judgment. The Exact Match metric correlated the most with manual evaluations, highlighting the importance of exactness in code completion tasks.

For future work, combining multiple metrics or developing new ones tailored to code semantics might improve the correlation with human judgment.

## References

- **StarCoder Model**: [GitHub Repository](https://github.com/bigcode-project/starcoder)
- **Evaluation Metrics**:
  - **BLEU Score**: Papineni, Kishore, et al. "BLEU: a method for automatic evaluation of machine translation."
  - **ROUGE Score**: Lin, Chin-Yew. "ROUGE: A Package for Automatic Evaluation of Summaries."
  - **chrF Score**: PopoviÄ‡, Maja. "chrF: character n-gram F-score for automatic MT evaluation."
  - **Levenshtein Distance**: Levenshtein, Vladimir I. "Binary codes capable of correcting deletions, insertions, and reversals."
- **Libraries Used**:
  - `transformers`: Hugging Face Transformers library.
  - `sacrebleu`: An open-source Python library for BLEU, chrF, and other metrics.
  - `python-Levenshtein`: A library for fast computation of Levenshtein distance and ratio.
  - `rouge-score`: A library for computing ROUGE scores.


